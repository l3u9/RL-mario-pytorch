{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOezpRkH7JytrrRtPp/vowR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/l3u9/RL-mario-pytorch/blob/main/mario.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 179,
      "metadata": {
        "id": "xEeOuYaGDbbr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65e50631-0909-407d-85d6-d05c30b3c05a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gym-super-mario-bros==7.4.0 in /usr/local/lib/python3.10/dist-packages (7.4.0)\n",
            "Requirement already satisfied: nes-py>=8.1.4 in /usr/local/lib/python3.10/dist-packages (from gym-super-mario-bros==7.4.0) (8.2.1)\n",
            "Requirement already satisfied: gym>=0.17.2 in /usr/local/lib/python3.10/dist-packages (from nes-py>=8.1.4->gym-super-mario-bros==7.4.0) (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from nes-py>=8.1.4->gym-super-mario-bros==7.4.0) (1.22.4)\n",
            "Requirement already satisfied: pyglet<=1.5.21,>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from nes-py>=8.1.4->gym-super-mario-bros==7.4.0) (1.5.21)\n",
            "Requirement already satisfied: tqdm>=4.48.2 in /usr/local/lib/python3.10/dist-packages (from nes-py>=8.1.4->gym-super-mario-bros==7.4.0) (4.65.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym>=0.17.2->nes-py>=8.1.4->gym-super-mario-bros==7.4.0) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym>=0.17.2->nes-py>=8.1.4->gym-super-mario-bros==7.4.0) (0.0.8)\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "pip install gym-super-mario-bros==7.4.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torchvision import transforms as T\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from collections import deque\n",
        "import random, datetime, os, copy\n",
        "\n",
        "import gym\n",
        "from gym.spaces import Box\n",
        "from gym.wrappers import FrameStack\n",
        "\n",
        "from nes_py.wrappers import JoypadSpace\n",
        "\n",
        "import gym_super_mario_bros"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jani57JzDsQh",
        "outputId": "13c7907e-0cf7-492c-eb6b-23d764e8a6db"
      },
      "execution_count": 180,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "강화학습의 개념\n",
        "\n",
        "Environment: 에이전트가 상호작용하며 스스로 배우는 세계\n",
        "\n",
        "Action a: 에이전트가 환경에 어떻게 응답하는지 행동을 나타낸다. 가능한 모든 행동의 집합을 행동 공간이라고 한다.\n",
        "\n",
        "State s: 환경의 현재 특성 상태를 통해 나타낸다. 환경이 있을 수 있는 모든 가능한 상태 집합을 상태 공간이라고 한다.\n",
        "\n",
        "Reward r: 포상은 환경에서 에이전트로 전달되는 피드백이다. 에이전트가 학습하고 향후 행동을 변경하도록 유도하는 것이다. 여러 시간 단계에 걸친 포상의 합을 return이라고 한다.\n",
        "\n",
        "Action-Value-function Q'(s, a): 상태 s에서 시작하면 예상되는 리턴을 반환하고 임의의 행동 a를 선택한다. 그 다음 각각의 미래의 단계에서 포상의 합을 극대화 하는 행동을 선택하도록 한다. Q는 상태에서 행동의 \"품질\"을 나타낸다."
      ],
      "metadata": {
        "id": "bZ5KPV9XENVa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Environment 초기화\n",
        "\n",
        "if gym.__version__ < '0.26':\n",
        "  env = gym_super_mario_bros.make('SuperMarioBros-1-1-v0', new_step_api=True)\n",
        "else:\n",
        "  env = gym_super_mario_bros.make('SuperMarioBros-1-1-v0', render_mode='rgb', apply_api_compatibility=True)\n",
        "\n",
        "env = JoypadSpace(env, [[\"right\"], [\"right\", 'A']])\n",
        "\n",
        "env.reset()\n",
        "next_state, reward, done, trunc, info = env.step(action=0)\n",
        "print(f\"{next_state.shape}, \\n {reward}, \\n {done}, \\n {info}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ysduzH-UE4KQ",
        "outputId": "4ffc963f-5bb8-4c3a-ab04-dfad4e9ecf7f"
      },
      "execution_count": 181,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/envs/registration.py:593: UserWarning: \u001b[33mWARN: The environment SuperMarioBros-1-1-v0 is out of date. You should consider upgrading to version `v3`.\u001b[0m\n",
            "  logger.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(240, 256, 3), \n",
            " 0.0, \n",
            " False, \n",
            " {'coins': 0, 'flag_get': False, 'life': 2, 'score': 0, 'stage': 1, 'status': 'small', 'time': 400, 'world': 1, 'x_pos': 40, 'y_pos': 79}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:227: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
            "  logger.deprecation(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 전처리\n",
        "class SkipFrame(gym.Wrapper):\n",
        "  def __init__(self, env, skip):\n",
        "    super().__init__(env)\n",
        "    self._skip = skip\n",
        "\n",
        "  def step(self, action):\n",
        "    total_reward = 0.0\n",
        "    for i in range(self._skip):\n",
        "      obs, reward, done, trunk, info = self.env.step(action)\n",
        "      total_reward += reward\n",
        "      if done:\n",
        "        break\n",
        "    return obs, total_reward, done, trunk, info\n",
        "\n",
        "\n",
        "class GrayScaleObservation(gym.ObservationWrapper):\n",
        "  def __init__(self, env):\n",
        "    super().__init__(env)\n",
        "    obs_shape = self.observation_space.shape[:2]\n",
        "    self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
        "\n",
        "  def permute_orientation(self, observation):\n",
        "    observation = np.transpose(observation, (2, 0, 1))\n",
        "    observation = torch.tensor(observation.copy(), dtype=torch.float)\n",
        "    return observation\n",
        "\n",
        "  def observation(self, observation):\n",
        "    observation = self.permute_orientation(observation)\n",
        "    transform = T.Grayscale()\n",
        "    observation = transform(observation)\n",
        "    return observation\n",
        "\n",
        "class ResizeObservation(gym.ObservationWrapper):\n",
        "  def __init__(self, env, shape):\n",
        "    super().__init__(env)\n",
        "    if isinstance(shape, int):\n",
        "      self.shape = (shape, shape)\n",
        "    else:\n",
        "      self.shape = tuple(shape)\n",
        "\n",
        "    obs_shape = self.shape + self.observation_space.shape[2:]\n",
        "\n",
        "    self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
        "\n",
        "  def observation(self, observation):\n",
        "    transforms = T.Compose([T.Resize(self.shape), T.Normalize(0, 255)])\n",
        "    observation = transforms(observation).squeeze(0)\n",
        "    return observation\n",
        "\n",
        "\n",
        "env = SkipFrame(env, skip=4)\n",
        "env = GrayScaleObservation(env)\n",
        "env = ResizeObservation(env, shape=84)\n",
        "\n",
        "if gym.__version__ < '0.26':\n",
        "  env = FrameStack(env, num_stack=4, new_step_api=True)\n",
        "else:\n",
        "  env = FrameStack(env, num_stack=4)\n",
        "\n"
      ],
      "metadata": {
        "id": "cUHZxGWMGcM8"
      },
      "execution_count": 182,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.modules.conv import Conv2d\n",
        "class MarioNet(nn.Module):\n",
        "\n",
        "  def __init__(self, input_dim, output_dim):\n",
        "    super().__init__()\n",
        "    c,h,w = input_dim\n",
        "\n",
        "    if h != 84:\n",
        "      raise ValueError(f\"Expecting input height: 84, got: {h}\")\n",
        "    if w!= 84:\n",
        "      raise ValueError(f\"Expecting input width: 84, got: {w}\")\n",
        "\n",
        "    self.online = nn.Sequential(\n",
        "        nn.Conv2d(in_channels=c, out_channels=32, kernel_size=8, stride=4),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1),\n",
        "        nn.ReLU(),\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(3136,512),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(512, output_dim),\n",
        "    )\n",
        "\n",
        "    self.target = copy.deepcopy(self.online)\n",
        "\n",
        "  def forward(self, input, model):\n",
        "    if model == \"online\":\n",
        "      return self.online(input)\n",
        "    elif model == \"target\":\n",
        "      return self.target(input)"
      ],
      "metadata": {
        "id": "B1tkSnqCIo32"
      },
      "execution_count": 183,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Mario:\n",
        "  def __init__(self, state_dim, action_dim, save_dir):\n",
        "    self.state_dim = state_dim\n",
        "    self.action_dim = action_dim\n",
        "    self.save_dir = save_dir\n",
        "\n",
        "    self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    self.net = MarioNet(self.state_dim, self.action_dim).float()\n",
        "    self.net = self.net.to(device=self.device)\n",
        "\n",
        "    self.exploration_rate = 1\n",
        "    self.exploration_rate_decay = 0.99999975\n",
        "    self.exploration_rate_min = 0.1\n",
        "\n",
        "    self.curr_step = 0\n",
        "\n",
        "    self.save_every = 5e5\n",
        "\n",
        "  def act(self, state):\n",
        "    if np.random.rand() < self.exploration_rate:\n",
        "      action_idx = np.random.randint(self.action_dim)\n",
        "    else:\n",
        "      state = state[0].__array__() if isinstance(state, tuple) else state.__array__()\n",
        "      state = torch.tensor(state, device=self.device).unsqueeze(0)\n",
        "      action_values = self.net(state, model=\"online\")\n",
        "      action_idx = torch.argmax(action_values, axis=1).item()\n",
        "\n",
        "    self.exploration_rate *= self.exploration_rate_decay\n",
        "    self.exploration_rate = max(self.exploration_rate_min, self.exploration_rate)\n",
        "\n",
        "    self.curr_step += 1\n",
        "    return action_idx\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vY8W45-SB_a-"
      },
      "execution_count": 184,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Mario(Mario):\n",
        "  def __init__(self, state_dim, action_dim, save_dir):\n",
        "    super().__init__(state_dim, action_dim, save_dir)\n",
        "    self.memory = deque(maxlen=100000)\n",
        "    self.batch_size = 32\n",
        "\n",
        "  def cache(self, state, next_state, action, reward, done):\n",
        "\n",
        "    def first_if_tuple(x):\n",
        "      return x[0] if isinstance(x, tuple) else x\n",
        "\n",
        "    state = first_if_tuple(state).__array__()\n",
        "    next_state = first_if_tuple(next_state).__array__()\n",
        "\n",
        "    state = torch.tensor(state, device=self.device)\n",
        "    next_state = torch.tensor(next_state, device=self.device)\n",
        "    action = torch.tensor([action], device=self.device)\n",
        "    reward = torch.tensor([reward], device=self.device)\n",
        "    done = torch.tensor([done], device=self.device)\n",
        "\n",
        "    self.memory.append((state, next_state, action, reward, done,))\n",
        "\n",
        "  def recall(self):\n",
        "    batch = random.sample(self.memory, self.batch_size)\n",
        "    state, next_state, action, reward, done = map(torch.stack, zip(*batch))\n",
        "    return state, next_state, action.squeeze(), reward.squeeze(), done.squeeze()\n",
        "\n"
      ],
      "metadata": {
        "id": "7JRciia_EBKa"
      },
      "execution_count": 185,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Mario(Mario):\n",
        "  def __init__(self, state_dim, action_dim, save_dir):\n",
        "    super().__init__(state_dim, action_dim, save_dir)\n",
        "    self.gamma = 0.9\n",
        "\n",
        "  def td_estimate(self, state, action):\n",
        "    current_Q = self.net(state, model=\"online\")[np.arange(0, self.batch_size), action]\n",
        "    return current_Q\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def td_target(self, reward, next_state, done):\n",
        "    next_state_Q = self.net(next_state, model=\"online\")\n",
        "    best_action = torch.argmax(next_state_Q, axis=1)\n",
        "    next_Q = self.net(next_state, model=\"target\")[\n",
        "        np.arange(0, self.batch_size), best_action\n",
        "        ]\n",
        "    return (reward + (1 - done.float()) * self.gamma * next_Q).float()"
      ],
      "metadata": {
        "id": "ib9bysTGOzpj"
      },
      "execution_count": 186,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Mario(Mario):\n",
        "  def __init__(self, state_dim, action_dim, save_dir):\n",
        "    super().__init__(state_dim, action_dim, save_dir)\n",
        "    self.optimizer = torch.optim.Adam(self.net.parameters(), lr=0.00025)\n",
        "    self.loss_fn = torch.nn.SmoothL1Loss()\n",
        "\n",
        "  def update_Q_online(self, td_estimate, td_target):\n",
        "    loss = self.loss_fn(td_estimate, td_target)\n",
        "    self.optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    self.optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "  def sync_Q_target(self):\n",
        "    self.net.target.load_state_dict(self.net.online.state_dict())"
      ],
      "metadata": {
        "id": "gw2Ar04Uwqdm"
      },
      "execution_count": 187,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Mario(Mario):\n",
        "  def save(self):\n",
        "    save_path = (\n",
        "        self.save_dir / f\"mario_net_{int(self.curr_step // self.save_every)}.chkpt\"\n",
        "    )\n",
        "    torch.save(\n",
        "        dict(model=self.net.state_dict(), exploration_rate=self.exploration_rate),\n",
        "        save_path,\n",
        "    )\n",
        "\n",
        "    print(f\"MarioNet saved to {save_path} at step {self.curr_step}\")\n"
      ],
      "metadata": {
        "id": "7VmmmbPw1cyG"
      },
      "execution_count": 188,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Mario(Mario):\n",
        "  def __init__(self, state_dim, action_dim, save_dir):\n",
        "    super().__init__(state_dim, action_dim, save_dir)\n",
        "    self.burnin = 1e4\n",
        "    self.learn_every = 3\n",
        "    self.sync_every = 1e4\n",
        "\n",
        "  def learn(self):\n",
        "    if self.curr_step % self.sync_every == 0:\n",
        "      self.sync_Q_target()\n",
        "\n",
        "    if self.curr_step % self.save_every == 0:\n",
        "      self.save()\n",
        "\n",
        "    if self.curr_step < self.burnin:\n",
        "      return None, None\n",
        "\n",
        "    state, next_state, action, reward, done = self.recall()\n",
        "\n",
        "    td_est = self.td_estimate(state, action)\n",
        "\n",
        "    td_tgt = self.td_target(reward, next_state, done)\n",
        "\n",
        "    loss = self.update_Q_online(td_est, td_tgt)\n",
        "\n",
        "    return (td_est.mean().item(), loss)"
      ],
      "metadata": {
        "id": "0LMZsyFK2FHf"
      },
      "execution_count": 189,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import time, datetime\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class MetricLogger:\n",
        "  def __init__(self, save_dir):\n",
        "    self.save_log = save_dir / \"log\"\n",
        "    with open(self.save_log, \"w\") as f:\n",
        "      f.write(\n",
        "                f\"{'Episode':>8}{'Step':>8}{'Epsilon':>10}{'MeanReward':>15}\"\n",
        "                f\"{'MeanLength':>15}{'MeanLoss':>15}{'MeanQValue':>15}\"\n",
        "                f\"{'TimeDelta':>15}{'Time':>20}\\n\"\n",
        "            )\n",
        "    self.ep_rewards_plot = save_dir / \"reward_plot.jpg\"\n",
        "    self.ep_lengths_plot = save_dir / \"length_plot.jpg\"\n",
        "    self.ep_avg_losses_plot = save_dir / \"loss_plot.jpg\"\n",
        "    self.ep_avg_qs_plot = save_dir / \"q_plot.jpg\"\n",
        "\n",
        "\n",
        "    self.ep_rewards = []\n",
        "    self.ep_lengths = []\n",
        "    self.ep_avg_losses = []\n",
        "    self.ep_avg_qs = []\n",
        "\n",
        "    self.moving_avg_ep_rewards = []\n",
        "    self.moving_avg_ep_lengths = []\n",
        "    self.moving_avg_ep_avg_losses = []\n",
        "    self.moving_avg_ep_avg_qs = []\n",
        "\n",
        "    self.init_episode()\n",
        "\n",
        "    self.record_time = time.time()\n",
        "\n",
        "  def log_step(self, reward, loss, q):\n",
        "    self.curr_ep_reward += reward\n",
        "    self.curr_ep_length += 1\n",
        "    if loss:\n",
        "      self.curr_ep_loss += loss\n",
        "      self.curr_ep_q += q\n",
        "      self.curr_ep_loss_length += 1\n",
        "\n",
        "  def log_episode(self):\n",
        "    self.ep_rewards.append(self.curr_ep_reward)\n",
        "    self.ep_lengths.append(self.curr_ep_length)\n",
        "    if self.curr_ep_loss_length == 0:\n",
        "      ep_avg_loss = 0\n",
        "      ep_avg_q = 0\n",
        "\n",
        "    else:\n",
        "      ep_avg_loss = np.round(self.curr_ep_loss / self.curr_ep_loss_length, 5)\n",
        "      ep_avg_q = np.round(self.curr_ep_q / self.curr_ep_loss_length, 5)\n",
        "    self.ep_avg_losses.append(ep_avg_loss)\n",
        "    self.ep_avg_qs.append(ep_avg_q)\n",
        "\n",
        "    self.init_episode()\n",
        "  def init_episode(self):\n",
        "        self.curr_ep_reward = 0.0\n",
        "        self.curr_ep_length = 0\n",
        "        self.curr_ep_loss = 0.0\n",
        "        self.curr_ep_q = 0.0\n",
        "        self.curr_ep_loss_length = 0\n",
        "\n",
        "  def record(self, episode, epsilon, step):\n",
        "    mean_ep_reward = np.round(np.mean(self.ep_rewards[-100:]), 3)\n",
        "    mean_ep_length = np.round(np.mean(self.ep_lengths[-100:]), 3)\n",
        "    mean_ep_loss = np.round(np.mean(self.ep_avg_losses[-100:]), 3)\n",
        "    mean_ep_q = np.round(np.mean(self.ep_avg_qs[-100:]), 3)\n",
        "    self.moving_avg_ep_rewards.append(mean_ep_reward)\n",
        "    self.moving_avg_ep_lengths.append(mean_ep_length)\n",
        "    self.moving_avg_ep_avg_losses.append(mean_ep_loss)\n",
        "    self.moving_avg_ep_avg_qs.append(mean_ep_q)\n",
        "\n",
        "    last_record_time = self.record_time\n",
        "    self.record_time = time.time()\n",
        "    time_since_last_record = np.round(self.record_time - last_record_time, 3)\n",
        "\n",
        "    print(\n",
        "        f\"Episode {episode} - \"\n",
        "        f\"Step {step} - \"\n",
        "        f\"Epsilon {epsilon} - \"\n",
        "        f\"Mean Reward {mean_ep_reward} - \"\n",
        "        f\"Mean Length {mean_ep_length} - \"\n",
        "        f\"Mean Loss {mean_ep_loss} - \"\n",
        "        f\"Mean Q Value {mean_ep_q} - \"\n",
        "        f\"Time Delta {time_since_last_record} - \"\n",
        "        f\"Time {datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S')}\"\n",
        "    )\n",
        "\n",
        "    with open(self.save_log, \"a\") as f:\n",
        "        f.write(\n",
        "            f\"{episode:8d}{step:8d}{epsilon:10.3f}\"\n",
        "            f\"{mean_ep_reward:15.3f}{mean_ep_length:15.3f}{mean_ep_loss:15.3f}{mean_ep_q:15.3f}\"\n",
        "            f\"{time_since_last_record:15.3f}\"\n",
        "            f\"{datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S'):>20}\\n\"\n",
        "        )\n",
        "\n",
        "    for metric in [\"ep_rewards\", \"ep_lengths\", \"ep_avg_losses\", \"ep_avg_qs\"]:\n",
        "        plt.plot(getattr(self, f\"moving_avg_{metric}\"))\n",
        "        plt.savefig(getattr(self, f\"{metric}_plot\"))\n",
        "        plt.clf()\n"
      ],
      "metadata": {
        "id": "E4jrR3P032Cb"
      },
      "execution_count": 196,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "use_cuda = torch.cuda.is_available()\n",
        "print(f\"Using CUDA: {use_cuda}\")\n",
        "print()\n",
        "\n",
        "save_dir = Path(\"checkpoints\") / datetime.datetime.now().strftime(\"%Y-%m-%dT%H-%M-%S\")\n",
        "save_dir.mkdir(parents=True)\n",
        "\n",
        "mario = Mario(state_dim=(4, 84, 84), action_dim=env.action_space.n, save_dir=save_dir)\n",
        "\n",
        "logger = MetricLogger(save_dir)\n",
        "\n",
        "episodes = 40000\n",
        "for e in range(episodes):\n",
        "\n",
        "    state = env.reset()\n",
        "\n",
        "    # 게임을 실행시켜봅시다!\n",
        "    while True:\n",
        "\n",
        "        # 현재 상태에서 에이전트 실행하기\n",
        "        action = mario.act(state)\n",
        "\n",
        "        # 에이전트가 액션 수행하기\n",
        "        next_state, reward, done, trunc, info = env.step(action)\n",
        "\n",
        "        # 기억하기\n",
        "        mario.cache(state, next_state, action, reward, done)\n",
        "\n",
        "        # 배우기\n",
        "        q, loss = mario.learn()\n",
        "\n",
        "        # 기록하기\n",
        "        logger.log_step(reward, loss, q)\n",
        "\n",
        "        # 상태 업데이트하기\n",
        "        state = next_state\n",
        "\n",
        "        # 게임이 끝났는지 확인하기\n",
        "        if done or info[\"flag_get\"]:\n",
        "            break\n",
        "\n",
        "    logger.log_episode()\n",
        "\n",
        "    if e % 20 == 0:\n",
        "        logger.record(episode=e, epsilon=mario.exploration_rate, step=mario.curr_step)"
      ],
      "metadata": {
        "id": "hJZ55Eqg5YmP",
        "outputId": "b2806a5c-5bd3-42eb-e05d-1bfd3e2cde39",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 517
        }
      },
      "execution_count": 198,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using CUDA: False\n",
            "\n",
            "Episode 0 - Step 107 - Epsilon 0.9999732503544305 - Mean Reward 604.0 - Mean Length 107.0 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 1.885 - Time 2023-07-20T06:30:12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-198-3c5a2d1ff200>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;31m# 에이전트가 액션 수행하기\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m# 기억하기\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gym/wrappers/frame_stack.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    175\u001b[0m         \"\"\"\n\u001b[1;32m    176\u001b[0m         observation, reward, terminated, truncated, info = step_api_compatibility(\n\u001b[0;32m--> 177\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m         )\n\u001b[1;32m    179\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gym/core.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m         \u001b[0;34m\"\"\"Returns a modified observation using :meth:`self.observation` after calling :meth:`env.step`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 483\u001b[0;31m         \u001b[0mstep_returns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    484\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_returns\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m             \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep_returns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gym/core.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m         \u001b[0;34m\"\"\"Returns a modified observation using :meth:`self.observation` after calling :meth:`env.step`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 483\u001b[0;31m         \u001b[0mstep_returns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    484\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_returns\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m             \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep_returns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-182-38267053fe29>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mtotal_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_skip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m       \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m       \u001b[0mtotal_reward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nes_py/wrappers/joypad_space.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \"\"\"\n\u001b[1;32m     73\u001b[0m         \u001b[0;31m# take the step and record the output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_action_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gym/wrappers/time_limit.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \"\"\"\n\u001b[1;32m     59\u001b[0m         observation, reward, terminated, truncated, info = step_api_compatibility(\n\u001b[0;32m---> 60\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m             \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gym/wrappers/order_enforcing.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_reset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mResetNeeded\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot call env.step() before calling env.reset()\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \"\"\"\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mstep_returns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_step_api\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mstep_to_new_api\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_returns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gym/wrappers/env_checker.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0menv_step_passive_checker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nes_py/nes_env.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    298\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrollers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m         \u001b[0;31m# pass the action to the emulator as an unsigned byte\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m         \u001b[0m_LIB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_env\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m         \u001b[0;31m# get the reward for this step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m         \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_reward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "o1vBSW2g8Baw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}