{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/l3u9/RL-mario-pytorch/blob/main/mario.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xEeOuYaGDbbr",
    "outputId": "65e50631-0909-407d-85d6-d05c30b3c05a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: gym-super-mario-bros==7.4.0 in /home/bu9/.local/lib/python3.8/site-packages (7.4.0)\n",
      "Requirement already satisfied: nes-py>=8.1.4 in /home/bu9/.local/lib/python3.8/site-packages (from gym-super-mario-bros==7.4.0) (8.2.1)\n",
      "Requirement already satisfied: gym>=0.17.2 in /home/bu9/.local/lib/python3.8/site-packages (from nes-py>=8.1.4->gym-super-mario-bros==7.4.0) (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /home/bu9/.local/lib/python3.8/site-packages (from nes-py>=8.1.4->gym-super-mario-bros==7.4.0) (1.24.4)\n",
      "Requirement already satisfied: pyglet<=1.5.21,>=1.4.0 in /home/bu9/.local/lib/python3.8/site-packages (from nes-py>=8.1.4->gym-super-mario-bros==7.4.0) (1.5.21)\n",
      "Requirement already satisfied: tqdm>=4.48.2 in /home/bu9/.local/lib/python3.8/site-packages (from nes-py>=8.1.4->gym-super-mario-bros==7.4.0) (4.65.0)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /home/bu9/.local/lib/python3.8/site-packages (from gym>=0.17.2->nes-py>=8.1.4->gym-super-mario-bros==7.4.0) (2.2.1)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in /home/bu9/.local/lib/python3.8/site-packages (from gym>=0.17.2->nes-py>=8.1.4->gym-super-mario-bros==7.4.0) (0.0.8)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in /home/bu9/.local/lib/python3.8/site-packages (from gym>=0.17.2->nes-py>=8.1.4->gym-super-mario-bros==7.4.0) (6.7.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/lib/python3/dist-packages (from importlib-metadata>=4.8.0->gym>=0.17.2->nes-py>=8.1.4->gym-super-mario-bros==7.4.0) (1.0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: distro-info 0.23ubuntu1 has a non-standard version number. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of distro-info or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\n",
      "DEPRECATION: python-debian 0.1.36ubuntu1 has a non-standard version number. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of python-debian or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "pip install gym-super-mario-bros==7.4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jani57JzDsQh",
    "outputId": "13c7907e-0cf7-492c-eb6b-23d764e8a6db"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import transforms as T\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from collections import deque\n",
    "import random, datetime, os, copy\n",
    "\n",
    "import gym\n",
    "from gym.spaces import Box\n",
    "from gym.wrappers import FrameStack\n",
    "\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "\n",
    "import gym_super_mario_bros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bZ5KPV9XENVa"
   },
   "source": [
    "강화학습의 개념\n",
    "\n",
    "Environment: 에이전트가 상호작용하며 스스로 배우는 세계\n",
    "\n",
    "Action a: 에이전트가 환경에 어떻게 응답하는지 행동을 나타낸다. 가능한 모든 행동의 집합을 행동 공간이라고 한다.\n",
    "\n",
    "State s: 환경의 현재 특성 상태를 통해 나타낸다. 환경이 있을 수 있는 모든 가능한 상태 집합을 상태 공간이라고 한다.\n",
    "\n",
    "Reward r: 포상은 환경에서 에이전트로 전달되는 피드백이다. 에이전트가 학습하고 향후 행동을 변경하도록 유도하는 것이다. 여러 시간 단계에 걸친 포상의 합을 return이라고 한다.\n",
    "\n",
    "Action-Value-function Q'(s, a): 상태 s에서 시작하면 예상되는 리턴을 반환하고 임의의 행동 a를 선택한다. 그 다음 각각의 미래의 단계에서 포상의 합을 극대화 하는 행동을 선택하도록 한다. Q는 상태에서 행동의 \"품질\"을 나타낸다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ysduzH-UE4KQ",
    "outputId": "4ffc963f-5bb8-4c3a-ab04-dfad4e9ecf7f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(240, 256, 3), \n",
      " 0.0, \n",
      " False, \n",
      " {'coins': 0, 'flag_get': False, 'life': 2, 'score': 0, 'stage': 1, 'status': 'small', 'time': 400, 'world': 1, 'x_pos': 40, 'y_pos': 79}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ksj31\\anaconda3\\lib\\site-packages\\gym\\envs\\registration.py:555: UserWarning: \u001b[33mWARN: The environment SuperMarioBros-1-1-v0 is out of date. You should consider upgrading to version `v3`.\u001b[0m\n",
      "  logger.warn(\n",
      "C:\\Users\\ksj31\\anaconda3\\lib\\site-packages\\gym\\envs\\registration.py:627: UserWarning: \u001b[33mWARN: The environment creator metadata doesn't include `render_modes`, contains: ['render.modes', 'video.frames_per_second']\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "# Environment 초기화\n",
    "\n",
    "if gym.__version__ < '0.26':\n",
    "  env = gym_super_mario_bros.make('SuperMarioBros-1-1-v0', new_step_api=True)\n",
    "else:\n",
    "  env = gym_super_mario_bros.make('SuperMarioBros-1-1-v0', render_mode='rgb', apply_api_compatibility=True)\n",
    "\n",
    "env = JoypadSpace(env, [[\"right\"], [\"right\", 'A']])\n",
    "\n",
    "env.reset()\n",
    "next_state, reward, done, trunc, info = env.step(action=0)\n",
    "print(f\"{next_state.shape}, \\n {reward}, \\n {done}, \\n {info}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "cUHZxGWMGcM8"
   },
   "outputs": [],
   "source": [
    "# 전처리\n",
    "class SkipFrame(gym.Wrapper):\n",
    "  def __init__(self, env, skip):\n",
    "    super().__init__(env)\n",
    "    self._skip = skip\n",
    "\n",
    "  def step(self, action):\n",
    "    total_reward = 0.0\n",
    "    for i in range(self._skip):\n",
    "      obs, reward, done, trunk, info = self.env.step(action)\n",
    "      total_reward += reward\n",
    "      if done:\n",
    "        break\n",
    "    return obs, total_reward, done, trunk, info\n",
    "\n",
    "\n",
    "class GrayScaleObservation(gym.ObservationWrapper):\n",
    "  def __init__(self, env):\n",
    "    super().__init__(env)\n",
    "    obs_shape = self.observation_space.shape[:2]\n",
    "    self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
    "\n",
    "  def permute_orientation(self, observation):\n",
    "    observation = np.transpose(observation, (2, 0, 1))\n",
    "    observation = torch.tensor(observation.copy(), dtype=torch.float)\n",
    "    return observation\n",
    "\n",
    "  def observation(self, observation):\n",
    "    observation = self.permute_orientation(observation)\n",
    "    transform = T.Grayscale()\n",
    "    observation = transform(observation)\n",
    "    return observation\n",
    "\n",
    "class ResizeObservation(gym.ObservationWrapper):\n",
    "  def __init__(self, env, shape):\n",
    "    super().__init__(env)\n",
    "    if isinstance(shape, int):\n",
    "      self.shape = (shape, shape)\n",
    "    else:\n",
    "      self.shape = tuple(shape)\n",
    "\n",
    "    obs_shape = self.shape + self.observation_space.shape[2:]\n",
    "\n",
    "    self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
    "\n",
    "  def observation(self, observation):\n",
    "    transforms = T.Compose([T.Resize(self.shape), T.Normalize(0, 255)])\n",
    "    observation = transforms(observation).squeeze(0)\n",
    "    return observation\n",
    "\n",
    "\n",
    "env = SkipFrame(env, skip=4)\n",
    "env = GrayScaleObservation(env)\n",
    "env = ResizeObservation(env, shape=84)\n",
    "\n",
    "if gym.__version__ < '0.26':\n",
    "  env = FrameStack(env, num_stack=4, new_step_api=True)\n",
    "else:\n",
    "  env = FrameStack(env, num_stack=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "B1tkSnqCIo32"
   },
   "outputs": [],
   "source": [
    "from torch.nn.modules.conv import Conv2d\n",
    "class MarioNet(nn.Module):\n",
    "\n",
    "  def __init__(self, input_dim, output_dim):\n",
    "    super().__init__()\n",
    "    c,h,w = input_dim\n",
    "\n",
    "    if h != 84:\n",
    "      raise ValueError(f\"Expecting input height: 84, got: {h}\")\n",
    "    if w!= 84:\n",
    "      raise ValueError(f\"Expecting input width: 84, got: {w}\")\n",
    "\n",
    "    self.online = nn.Sequential(\n",
    "        nn.Conv2d(in_channels=c, out_channels=32, kernel_size=8, stride=4),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1),\n",
    "        nn.ReLU(),\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(3136,512),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(512, output_dim),\n",
    "    )\n",
    "\n",
    "    self.target = copy.deepcopy(self.online)\n",
    "\n",
    "  def forward(self, input, model):\n",
    "    if model == \"online\":\n",
    "      return self.online(input)\n",
    "    elif model == \"target\":\n",
    "      return self.target(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "vY8W45-SB_a-"
   },
   "outputs": [],
   "source": [
    "class Mario:\n",
    "  def __init__(self, state_dim, action_dim, save_dir):\n",
    "    self.state_dim = state_dim\n",
    "    self.action_dim = action_dim\n",
    "    self.save_dir = save_dir\n",
    "\n",
    "    self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    self.net = MarioNet(self.state_dim, self.action_dim).float()\n",
    "    self.net = self.net.to(device=self.device)\n",
    "\n",
    "    self.exploration_rate = 1\n",
    "    self.exploration_rate_decay = 0.99999975\n",
    "    self.exploration_rate_min = 0.1\n",
    "\n",
    "    self.curr_step = 0\n",
    "\n",
    "    self.save_every = 5e5\n",
    "\n",
    "  def act(self, state):\n",
    "    if np.random.rand() < self.exploration_rate:\n",
    "      action_idx = np.random.randint(self.action_dim)\n",
    "    else:\n",
    "      state = state[0].__array__() if isinstance(state, tuple) else state.__array__()\n",
    "      state = torch.tensor(state, device=self.device).unsqueeze(0)\n",
    "      action_values = self.net(state, model=\"online\")\n",
    "      action_idx = torch.argmax(action_values, axis=1).item()\n",
    "\n",
    "    self.exploration_rate *= self.exploration_rate_decay\n",
    "    self.exploration_rate = max(self.exploration_rate_min, self.exploration_rate)\n",
    "\n",
    "    self.curr_step += 1\n",
    "    return action_idx\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "7JRciia_EBKa"
   },
   "outputs": [],
   "source": [
    "class Mario(Mario):\n",
    "  def __init__(self, state_dim, action_dim, save_dir):\n",
    "    super().__init__(state_dim, action_dim, save_dir)\n",
    "    self.memory = deque(maxlen=100000)\n",
    "    self.batch_size = 32\n",
    "\n",
    "  def cache(self, state, next_state, action, reward, done):\n",
    "\n",
    "    def first_if_tuple(x):\n",
    "      return x[0] if isinstance(x, tuple) else x\n",
    "\n",
    "    state = first_if_tuple(state).__array__()\n",
    "    next_state = first_if_tuple(next_state).__array__()\n",
    "\n",
    "    state = torch.tensor(state, device=self.device)\n",
    "    next_state = torch.tensor(next_state, device=self.device)\n",
    "    action = torch.tensor([action], device=self.device)\n",
    "    reward = torch.tensor([reward], device=self.device)\n",
    "    done = torch.tensor([done], device=self.device)\n",
    "\n",
    "    self.memory.append((state, next_state, action, reward, done,))\n",
    "\n",
    "  def recall(self):\n",
    "    batch = random.sample(self.memory, self.batch_size)\n",
    "    state, next_state, action, reward, done = map(torch.stack, zip(*batch))\n",
    "    return state, next_state, action.squeeze(), reward.squeeze(), done.squeeze()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "ib9bysTGOzpj"
   },
   "outputs": [],
   "source": [
    "class Mario(Mario):\n",
    "  def __init__(self, state_dim, action_dim, save_dir):\n",
    "    super().__init__(state_dim, action_dim, save_dir)\n",
    "    self.gamma = 0.9\n",
    "\n",
    "  def td_estimate(self, state, action):\n",
    "    current_Q = self.net(state, model=\"online\")[np.arange(0, self.batch_size), action]\n",
    "    return current_Q\n",
    "\n",
    "  @torch.no_grad()\n",
    "  def td_target(self, reward, next_state, done):\n",
    "    next_state_Q = self.net(next_state, model=\"online\")\n",
    "    best_action = torch.argmax(next_state_Q, axis=1)\n",
    "    next_Q = self.net(next_state, model=\"target\")[\n",
    "        np.arange(0, self.batch_size), best_action\n",
    "        ]\n",
    "    return (reward + (1 - done.float()) * self.gamma * next_Q).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "gw2Ar04Uwqdm"
   },
   "outputs": [],
   "source": [
    "class Mario(Mario):\n",
    "  def __init__(self, state_dim, action_dim, save_dir):\n",
    "    super().__init__(state_dim, action_dim, save_dir)\n",
    "    self.optimizer = torch.optim.Adam(self.net.parameters(), lr=0.00025)\n",
    "    self.loss_fn = torch.nn.SmoothL1Loss()\n",
    "\n",
    "  def update_Q_online(self, td_estimate, td_target):\n",
    "    loss = self.loss_fn(td_estimate, td_target)\n",
    "    self.optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    self.optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "  def sync_Q_target(self):\n",
    "    self.net.target.load_state_dict(self.net.online.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "7VmmmbPw1cyG"
   },
   "outputs": [],
   "source": [
    "class Mario(Mario):\n",
    "  def save(self):\n",
    "    save_path = (\n",
    "        self.save_dir / f\"mario_net_{int(self.curr_step // self.save_every)}.chkpt\"\n",
    "    )\n",
    "    torch.save(\n",
    "        dict(model=self.net.state_dict(), exploration_rate=self.exploration_rate),\n",
    "        save_path,\n",
    "    )\n",
    "\n",
    "    print(f\"MarioNet saved to {save_path} at step {self.curr_step}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "0LMZsyFK2FHf"
   },
   "outputs": [],
   "source": [
    "class Mario(Mario):\n",
    "  def __init__(self, state_dim, action_dim, save_dir):\n",
    "    super().__init__(state_dim, action_dim, save_dir)\n",
    "    self.burnin = 1e4\n",
    "    self.learn_every = 3\n",
    "    self.sync_every = 1e4\n",
    "\n",
    "  def learn(self):\n",
    "    if self.curr_step % self.sync_every == 0:\n",
    "      self.sync_Q_target()\n",
    "\n",
    "    if self.curr_step % self.save_every == 0:\n",
    "      self.save()\n",
    "\n",
    "    if self.curr_step < self.burnin:\n",
    "      return None, None\n",
    "\n",
    "    state, next_state, action, reward, done = self.recall()\n",
    "\n",
    "    td_est = self.td_estimate(state, action)\n",
    "\n",
    "    td_tgt = self.td_target(reward, next_state, done)\n",
    "\n",
    "    loss = self.update_Q_online(td_est, td_tgt)\n",
    "\n",
    "    return (td_est.mean().item(), loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "E4jrR3P032Cb"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time, datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class MetricLogger:\n",
    "  def __init__(self, save_dir):\n",
    "    self.save_log = save_dir / \"log\"\n",
    "    with open(self.save_log, \"w\") as f:\n",
    "      f.write(\n",
    "                f\"{'Episode':>8}{'Step':>8}{'Epsilon':>10}{'MeanReward':>15}\"\n",
    "                f\"{'MeanLength':>15}{'MeanLoss':>15}{'MeanQValue':>15}\"\n",
    "                f\"{'TimeDelta':>15}{'Time':>20}\\n\"\n",
    "            )\n",
    "    self.ep_rewards_plot = save_dir / \"reward_plot.jpg\"\n",
    "    self.ep_lengths_plot = save_dir / \"length_plot.jpg\"\n",
    "    self.ep_avg_losses_plot = save_dir / \"loss_plot.jpg\"\n",
    "    self.ep_avg_qs_plot = save_dir / \"q_plot.jpg\"\n",
    "\n",
    "\n",
    "    self.ep_rewards = []\n",
    "    self.ep_lengths = []\n",
    "    self.ep_avg_losses = []\n",
    "    self.ep_avg_qs = []\n",
    "\n",
    "    self.moving_avg_ep_rewards = []\n",
    "    self.moving_avg_ep_lengths = []\n",
    "    self.moving_avg_ep_avg_losses = []\n",
    "    self.moving_avg_ep_avg_qs = []\n",
    "\n",
    "    self.init_episode()\n",
    "\n",
    "    self.record_time = time.time()\n",
    "\n",
    "  def log_step(self, reward, loss, q):\n",
    "    self.curr_ep_reward += reward\n",
    "    self.curr_ep_length += 1\n",
    "    if loss:\n",
    "      self.curr_ep_loss += loss\n",
    "      self.curr_ep_q += q\n",
    "      self.curr_ep_loss_length += 1\n",
    "\n",
    "  def log_episode(self):\n",
    "    self.ep_rewards.append(self.curr_ep_reward)\n",
    "    self.ep_lengths.append(self.curr_ep_length)\n",
    "    if self.curr_ep_loss_length == 0:\n",
    "      ep_avg_loss = 0\n",
    "      ep_avg_q = 0\n",
    "\n",
    "    else:\n",
    "      ep_avg_loss = np.round(self.curr_ep_loss / self.curr_ep_loss_length, 5)\n",
    "      ep_avg_q = np.round(self.curr_ep_q / self.curr_ep_loss_length, 5)\n",
    "    self.ep_avg_losses.append(ep_avg_loss)\n",
    "    self.ep_avg_qs.append(ep_avg_q)\n",
    "\n",
    "    self.init_episode()\n",
    "  def init_episode(self):\n",
    "        self.curr_ep_reward = 0.0\n",
    "        self.curr_ep_length = 0\n",
    "        self.curr_ep_loss = 0.0\n",
    "        self.curr_ep_q = 0.0\n",
    "        self.curr_ep_loss_length = 0\n",
    "\n",
    "  def record(self, episode, epsilon, step):\n",
    "    mean_ep_reward = np.round(np.mean(self.ep_rewards[-100:]), 3)\n",
    "    mean_ep_length = np.round(np.mean(self.ep_lengths[-100:]), 3)\n",
    "    mean_ep_loss = np.round(np.mean(self.ep_avg_losses[-100:]), 3)\n",
    "    mean_ep_q = np.round(np.mean(self.ep_avg_qs[-100:]), 3)\n",
    "    self.moving_avg_ep_rewards.append(mean_ep_reward)\n",
    "    self.moving_avg_ep_lengths.append(mean_ep_length)\n",
    "    self.moving_avg_ep_avg_losses.append(mean_ep_loss)\n",
    "    self.moving_avg_ep_avg_qs.append(mean_ep_q)\n",
    "\n",
    "    last_record_time = self.record_time\n",
    "    self.record_time = time.time()\n",
    "    time_since_last_record = np.round(self.record_time - last_record_time, 3)\n",
    "\n",
    "    print(\n",
    "        f\"Episode {episode} - \"\n",
    "        f\"Step {step} - \"\n",
    "        f\"Epsilon {epsilon} - \"\n",
    "        f\"Mean Reward {mean_ep_reward} - \"\n",
    "        f\"Mean Length {mean_ep_length} - \"\n",
    "        f\"Mean Loss {mean_ep_loss} - \"\n",
    "        f\"Mean Q Value {mean_ep_q} - \"\n",
    "        f\"Time Delta {time_since_last_record} - \"\n",
    "        f\"Time {datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S')}\"\n",
    "    )\n",
    "\n",
    "    with open(self.save_log, \"a\") as f:\n",
    "        f.write(\n",
    "            f\"{episode:8d}{step:8d}{epsilon:10.3f}\"\n",
    "            f\"{mean_ep_reward:15.3f}{mean_ep_length:15.3f}{mean_ep_loss:15.3f}{mean_ep_q:15.3f}\"\n",
    "            f\"{time_since_last_record:15.3f}\"\n",
    "            f\"{datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S'):>20}\\n\"\n",
    "        )\n",
    "\n",
    "    for metric in [\"ep_rewards\", \"ep_lengths\", \"ep_avg_losses\", \"ep_avg_qs\"]:\n",
    "        plt.plot(getattr(self, f\"moving_avg_{metric}\"))\n",
    "        plt.savefig(getattr(self, f\"{metric}_plot\"))\n",
    "        plt.clf()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 517
    },
    "id": "hJZ55Eqg5YmP",
    "outputId": "b2806a5c-5bd3-42eb-e05d-1bfd3e2cde39"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA: True\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ksj31\\anaconda3\\lib\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 - Step 804 - Epsilon 0.9997990201739976 - Mean Reward 922.0 - Mean Length 804.0 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 3.636 - Time 2023-07-20T15:35:06\n"
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "print(f\"Using CUDA: {use_cuda}\")\n",
    "print()\n",
    "\n",
    "save_dir = Path(\"checkpoints\") / datetime.datetime.now().strftime(\"%Y-%m-%dT%H-%M-%S\")\n",
    "save_dir.mkdir(parents=True)\n",
    "\n",
    "mario = Mario(state_dim=(4, 84, 84), action_dim=env.action_space.n, save_dir=save_dir)\n",
    "\n",
    "logger = MetricLogger(save_dir)\n",
    "\n",
    "episodes = 40000\n",
    "for e in range(episodes):\n",
    "\n",
    "    state = env.reset()\n",
    "\n",
    "    # 게임을 실행시켜봅시다!\n",
    "    while True:\n",
    "\n",
    "        # 현재 상태에서 에이전트 실행하기\n",
    "        action = mario.act(state)\n",
    "\n",
    "        # 에이전트가 액션 수행하기\n",
    "        next_state, reward, done, trunc, info = env.step(action)\n",
    "\n",
    "        # 기억하기\n",
    "        mario.cache(state, next_state, action, reward, done)\n",
    "\n",
    "        # 배우기\n",
    "        q, loss = mario.learn()\n",
    "\n",
    "        # 기록하기\n",
    "        logger.log_step(reward, loss, q)\n",
    "\n",
    "        # 상태 업데이트하기\n",
    "        state = next_state\n",
    "\n",
    "        # 게임이 끝났는지 확인하기\n",
    "        if done or info[\"flag_get\"]:\n",
    "            break\n",
    "\n",
    "    logger.log_episode()\n",
    "\n",
    "    if e % 20 == 0:\n",
    "        logger.record(episode=e, epsilon=mario.exploration_rate, step=mario.curr_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o1vBSW2g8Baw"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOezpRkH7JytrrRtPp/vowR",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
